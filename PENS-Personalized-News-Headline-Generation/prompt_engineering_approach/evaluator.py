"""
è¯„ä¼°æ¨¡å— - åŒ…å«å¤§æ¨¡å‹APIè¯„ä»·å’Œä¸ªæ€§åŒ–æ•ˆæœè¯„ä¼°
"""

import os
import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import re

# ä¿®å¤ROUGEå¯¼å…¥
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: rouge_scoreåŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•")
    ROUGE_AVAILABLE = False

from config import EVALUATION_CONFIG, DATA_PATHS, EVALUATION_MODEL
from llm_client import LLMClient

class Evaluator:
    """è¯„ä¼°å™¨"""
    
    def __init__(self, use_llm_evaluation: bool = True):
        self.logger = self._setup_logger()
        self.rouge_scorer = None
        self.use_llm_evaluation = use_llm_evaluation
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ç”¨äºè¯„ä¼°
        if use_llm_evaluation:
            # ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„è¯„ä¼°æ¨¡å‹åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
            self.llm_client = LLMClient(EVALUATION_MODEL)
            self.logger.info(f"ä½¿ç”¨ {EVALUATION_MODEL} è¿›è¡ŒLLMè¯„ä¼°")
        
        # åˆå§‹åŒ–ROUGEè¯„ä¼°å™¨
        if ROUGE_AVAILABLE:
            try:
                self.rouge_scorer = rouge_scorer.RougeScorer(
                    ['rouge1', 'rouge2', 'rougeL'], 
                    use_stemmer=True
                )
                self.logger.info("ROUGEè¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"ROUGEè¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.rouge_scorer = None
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.evaluation_results = {}
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def calculate_rouge_scores(self, generated_titles: List[str], reference_titles: List[str]) -> Dict[str, float]:
        """è®¡ç®—ROUGEåˆ†æ•°"""
        
        if not self.rouge_scorer:
            self.logger.warning("ROUGEè¯„ä¼°å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è¯„ä¼°")
            return self._calculate_simple_scores(generated_titles, reference_titles)
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            processed_generated = [self._preprocess_text_for_rouge(title) for title in generated_titles]
            processed_reference = [self._preprocess_text_for_rouge(title) for title in reference_titles]
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬
            valid_pairs = [(g, r) for g, r in zip(processed_generated, processed_reference) 
                          if g and r and g != "empty text" and r != "empty text"]
            
            if not valid_pairs:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å¯¹è¿›è¡ŒROUGEè¯„ä¼°")
                return {}
            
            # è®¡ç®—ROUGEåˆ†æ•°
            rouge1_scores = []
            rouge2_scores = []
            rougeL_scores = []
            
            for generated, reference in valid_pairs:
                scores = self.rouge_scorer.score(reference, generated)
                rouge1_scores.append(scores['rouge1'])
                rouge2_scores.append(scores['rouge2'])
                rougeL_scores.append(scores['rougeL'])
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            rouge_results = {
                'rouge1_f': np.mean([score.fmeasure for score in rouge1_scores]),
                'rouge1_p': np.mean([score.precision for score in rouge1_scores]),
                'rouge1_r': np.mean([score.recall for score in rouge1_scores]),
                'rouge2_f': np.mean([score.fmeasure for score in rouge2_scores]),
                'rouge2_p': np.mean([score.precision for score in rouge2_scores]),
                'rouge2_r': np.mean([score.recall for score in rouge2_scores]),
                'rougeL_f': np.mean([score.fmeasure for score in rougeL_scores]),
                'rougeL_p': np.mean([score.precision for score in rougeL_scores]),
                'rougeL_r': np.mean([score.recall for score in rougeL_scores]),
            }
            
            self.logger.info(f"ROUGEè¯„ä¼°å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_pairs)}")
            return rouge_results
            
        except Exception as e:
            self.logger.error(f"ROUGEè¯„ä¼°å¤±è´¥: {str(e)}")
            return self._calculate_simple_scores(generated_titles, reference_titles)
    
    def _preprocess_text_for_rouge(self, text: str) -> str:
        """ä¸ºROUGEè¯„ä¼°é¢„å¤„ç†æ–‡æœ¬"""
        if not text or not isinstance(text, str):
            return "empty text"
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        text = ' '.join(text.split())
        
        # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
        if not text.strip():
            return "empty text"
            
        return text.strip()
    
    def _calculate_simple_scores(self, generated_titles: List[str], reference_titles: List[str]) -> Dict[str, float]:
        """ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼ˆå½“ROUGEä¸å¯ç”¨æ—¶ï¼‰"""
        
        scores = {}
        total_pairs = len(generated_titles)
        
        if total_pairs == 0:
            return scores
        
        # è®¡ç®—å­—ç¬¦çº§åˆ«çš„é‡å 
        char_overlaps = []
        word_overlaps = []
        length_ratios = []
        
        for gen, ref in zip(generated_titles, reference_titles):
            if not gen or not ref:
                continue
                
            gen_chars = set(gen.lower())
            ref_chars = set(ref.lower())
            char_overlap = len(gen_chars.intersection(ref_chars)) / max(len(gen_chars.union(ref_chars)), 1)
            char_overlaps.append(char_overlap)
            
            gen_words = set(gen.lower().split())
            ref_words = set(ref.lower().split())
            word_overlap = len(gen_words.intersection(ref_words)) / max(len(gen_words.union(ref_words)), 1)
            word_overlaps.append(word_overlap)
            
            length_ratio = min(len(gen), len(ref)) / max(len(gen), len(ref), 1)
            length_ratios.append(length_ratio)
        
        scores['simple_char_overlap'] = np.mean(char_overlaps) if char_overlaps else 0.0
        scores['simple_word_overlap'] = np.mean(word_overlaps) if word_overlaps else 0.0
        scores['length_similarity'] = np.mean(length_ratios) if length_ratios else 0.0
        
        return scores
    
    def evaluate_personalization(self, generated_titles: List[str], 
                                        user_interests: List[Dict], 
                                        news_categories: List[str],
                                        user_histories: List[List[str]]) -> Dict[str, float]:
        """ä¸ªæ€§åŒ–æ•ˆæœè¯„ä¼°ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        
        personalization_scores = []
        category_relevance_scores = []
        interest_alignment_scores = []
        history_relevance_scores = []
        
        for i, (title, interests, category, history) in enumerate(
            zip(generated_titles, user_interests, news_categories, user_histories)):
            
            if not title or not isinstance(interests, dict):
                continue
            
            title_lower = title.lower()
            
            # 1. å…´è¶£åŒ¹é…åº¦è¯„ä¼°ï¼ˆæ”¹è¿›é€»è¾‘ï¼‰
            primary_interest = interests.get('primary_interest', '').lower()
            interest_categories = [cat.lower() for cat in interests.get('categories', [])]
            
            interest_score = 0.0
            
            # ç±»åˆ«ç›´æ¥åŒ¹é…ï¼ˆæƒé‡60%ï¼‰
            if category and category.lower() in interest_categories:
                interest_score += 0.6
            
            # ä¸»è¦å…´è¶£åŒ¹é…ï¼ˆæƒé‡25%ï¼‰
            if primary_interest and primary_interest in title_lower:
                interest_score += 0.25
            
            # å…³é”®è¯éƒ¨åˆ†åŒ¹é…ï¼ˆæƒé‡15%ï¼‰
            if primary_interest:
                primary_words = primary_interest.split()
                matched_words = sum(1 for word in primary_words if len(word) > 3 and word in title_lower)
                if primary_words:
                    interest_score += 0.15 * (matched_words / len(primary_words))
            
            personalization_scores.append(min(interest_score, 1.0))
            
            # 2. ç±»åˆ«ç›¸å…³æ€§è¯„ä¼°ï¼ˆæ”¹è¿›ï¼‰
            category_score = 0.0
            if category and interest_categories:
                if category.lower() in interest_categories:
                    category_score = 1.0  # ç›´æ¥åŒ¹é…
                else:
                    # è¯­ä¹‰ç›¸å…³æ€§æ£€æŸ¥
                    category_words = set(category.lower().split())
                    for cat in interest_categories:
                        cat_words = set(cat.split())
                        if category_words.intersection(cat_words):
                            category_score = max(category_score, 0.5)
            
            category_relevance_scores.append(category_score)
            
            # 3. å…´è¶£ä¸€è‡´æ€§è¯„ä¼°ï¼ˆæ›´åˆç†ï¼‰
            alignment_score = 0.0
            if category and interest_categories:
                if category.lower() in interest_categories:
                    alignment_score = 1.0
                elif 'news' in interest_categories and category in ['news', 'politics', 'business']:
                    alignment_score = 0.8  # æ–°é—»ç›¸å…³ç±»åˆ«
                elif 'finance' in interest_categories and category in ['business', 'finance', 'economy']:
                    alignment_score = 0.8  # é‡‘èç›¸å…³ç±»åˆ«
                else:
                    alignment_score = 0.4  # åŸºç¡€åˆ†
            
            interest_alignment_scores.append(alignment_score)
            
            # 4. å†å²ç›¸å…³æ€§è¯„ä¼°ï¼ˆæ”¹è¿›ï¼‰
            history_score = 0.0
            if history and len(history) > 0:
                title_words = set(title_lower.split())
                history_words = set()
                for hist_title in history[:15]:  # çœ‹æ›´å¤šå†å²
                    if hist_title:
                        history_words.update(hist_title.lower().split())
                
                if history_words:
                    common_words = title_words.intersection(history_words)
                    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
                    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an'}
                    meaningful_words = [w for w in common_words if len(w) > 3 and w not in stop_words]
                    if title_words:
                        history_score = min(len(meaningful_words) / len(title_words), 1.0)
            
            history_relevance_scores.append(history_score)
        
        results = {
            'rule_based_personalization': np.mean(personalization_scores) if personalization_scores else 0.0,
            'rule_based_category_relevance': np.mean(category_relevance_scores) if category_relevance_scores else 0.0,
            'rule_based_interest_alignment': np.mean(interest_alignment_scores) if interest_alignment_scores else 0.0,
            'rule_based_history_relevance': np.mean(history_relevance_scores) if history_relevance_scores else 0.0
        }
        
        # è®¡ç®—ç»¼åˆè§„åˆ™è¯„åˆ†
        results['rule_based_overall'] = (
            results['rule_based_personalization'] * 0.4 +
            results['rule_based_category_relevance'] * 0.25 +
            results['rule_based_interest_alignment'] * 0.25 +
            results['rule_based_history_relevance'] * 0.1
        )
        
        return results
    
    def llm_evaluate_personalization(self, generated_titles: List[str], 
                                   user_interests: List[Dict], 
                                   news_categories: List[str],
                                   user_histories: List[List[str]], 
                                   batch_size: int = 5) -> Dict[str, float]:
        """ä½¿ç”¨LLMè¯„ä¼°ä¸ªæ€§åŒ–æ•ˆæœ"""
        
        if not self.use_llm_evaluation or not self.llm_client:
            self.logger.warning("LLMä¸ªæ€§åŒ–è¯„ä¼°ä¸å¯ç”¨")
            return {}
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸ªæ€§åŒ–æ–°é—»æ¨èè¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æ–°é—»æ ‡é¢˜çš„ä¸ªæ€§åŒ–ç¨‹åº¦è¿›è¡Œè¯„åˆ†ã€‚

è¯„ä¼°ç»´åº¦ (æ¯ä¸ªç»´åº¦0-10åˆ†):
1. å…´è¶£åŒ¹é…åº¦: æ ‡é¢˜å†…å®¹ä¸ç”¨æˆ·å…´è¶£çš„åŒ¹é…ç¨‹åº¦
2. ç±»åˆ«ç›¸å…³æ€§: æ ‡é¢˜ä¸ç”¨æˆ·åå¥½ç±»åˆ«çš„ç›¸å…³æ€§  
3. å†å²ä¸€è‡´æ€§: æ ‡é¢˜ä¸ç”¨æˆ·å†å²é˜…è¯»ä¹ æƒ¯çš„ä¸€è‡´æ€§
4. ä¸ªæ€§åŒ–åˆ›æ–°: æ ‡é¢˜åœ¨ä¸ªæ€§åŒ–æ–¹é¢çš„åˆ›æ–°å’Œå¸å¼•åŠ›
5. ç»¼åˆä¸ªæ€§åŒ–ç¨‹åº¦: æ•´ä½“ä¸ªæ€§åŒ–æ•ˆæœ

ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºï¼š
- åªè¾“å‡ºæ•°å­—å’Œé€—å·ï¼Œä¸è¦ä»»ä½•æ–‡å­—è¯´æ˜
- æ¯è¡Œä¸€ä¸ªæ ‡é¢˜çš„è¯„åˆ†ï¼Œæ ¼å¼ï¼šåˆ†æ•°1,åˆ†æ•°2,åˆ†æ•°3,åˆ†æ•°4,åˆ†æ•°5
- åˆ†æ•°èŒƒå›´0-10ï¼Œå¯ä»¥æ˜¯å°æ•°ï¼ˆå¦‚8.5ï¼‰
- æŒ‰ç…§æ ‡é¢˜é¡ºåºä¾æ¬¡è¾“å‡º
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€åˆ†ææˆ–å…¶ä»–æ–‡å­—

ç¤ºä¾‹è¾“å‡ºï¼š
8.5,7.0,9.0,6.5,8.0
6.0,8.5,5.5,7.0,6.5"""

        all_scores = {
            'interest_match': [],
            'category_relevance': [],
            'history_consistency': [],
            'personalization_innovation': [],
            'overall_personalization': []
        }
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(generated_titles), batch_size):
            batch_titles = generated_titles[i:i+batch_size]
            batch_interests = user_interests[i:i+batch_size]
            batch_categories = news_categories[i:i+batch_size]
            # ç¡®ä¿user_historiesä¸ä¸ºç©ºä¸”é•¿åº¦æ­£ç¡®
            if user_histories and len(user_histories) > i:
                batch_histories = user_histories[i:i+batch_size]
            else:
                batch_histories = [[] for _ in batch_titles]
            
            # æ„å»ºæ‰¹æ¬¡è¯„ä¼°æç¤ºè¯
            user_prompt = self._build_personalization_evaluation_prompt(
                batch_titles, batch_interests, batch_categories, batch_histories)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            try:
                response = self.llm_client.chat_completion(messages, max_tokens=1500)
                if response:
                    batch_scores = self._parse_personalization_evaluation_response(response)
                    for j, scores in enumerate(batch_scores):
                        if len(scores) >= 5:
                            all_scores['interest_match'].append(scores[0])
                            all_scores['category_relevance'].append(scores[1])
                            all_scores['history_consistency'].append(scores[2])
                            all_scores['personalization_innovation'].append(scores[3])
                            all_scores['overall_personalization'].append(scores[4])
                        else:
                            # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                            for key in all_scores:
                                all_scores[key].append(5.0)
                else:
                    # æ·»åŠ é»˜è®¤åˆ†æ•°
                    for _ in range(len(batch_titles)):
                        for key in all_scores:
                            all_scores[key].append(5.0)
                            
            except Exception as e:
                self.logger.error(f"LLMä¸ªæ€§åŒ–è¯„ä¼°å¤±è´¥: {e}")
                for _ in range(len(batch_titles)):
                    for key in all_scores:
                        all_scores[key].append(5.0)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆè½¬æ¢ä¸º0-1èŒƒå›´ï¼‰
        results = {}
        for key, scores in all_scores.items():
            if scores:
                results[f'llm_{key}'] = np.mean(scores) / 10.0  # è½¬æ¢ä¸º0-1èŒƒå›´
                results[f'llm_{key}_std'] = np.std(scores) / 10.0
        
        return results
    
    def _build_personalization_evaluation_prompt(self, titles: List[str], 
                                                interests: List[Dict], 
                                                categories: List[str],
                                                histories: List[List[str]]) -> str:
        """æ„å»ºä¸ªæ€§åŒ–è¯„ä¼°æç¤ºè¯"""
        
        prompt = "ä½œä¸ºä¸ªæ€§åŒ–æ–°é—»æ¨èè¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ¯ä¸ªæ–°é—»æ ‡é¢˜çš„ä¸ªæ€§åŒ–ç¨‹åº¦è¿›è¡Œè¯„åˆ†ã€‚\n\n"
        
        prompt += "ğŸ“‹ è¯„ä¼°ç»´åº¦ï¼ˆæ¯ä¸ªç»´åº¦0-10åˆ†ï¼‰ï¼š\n"
        prompt += "ç»´åº¦1: å…´è¶£åŒ¹é…åº¦ - æ ‡é¢˜ä¸ç”¨æˆ·å…´è¶£çš„åŒ¹é…ç¨‹åº¦\n"
        prompt += "ç»´åº¦2: ç±»åˆ«ç›¸å…³æ€§ - æ ‡é¢˜ä¸ç”¨æˆ·åå¥½ç±»åˆ«çš„ç›¸å…³æ€§\n"  
        prompt += "ç»´åº¦3: å†å²ä¸€è‡´æ€§ - æ ‡é¢˜ä¸ç”¨æˆ·å†å²é˜…è¯»çš„ä¸€è‡´æ€§\n"
        prompt += "ç»´åº¦4: ä¸ªæ€§åŒ–åˆ›æ–° - æ ‡é¢˜çš„ä¸ªæ€§åŒ–å¸å¼•åŠ›\n"
        prompt += "ç»´åº¦5: ç»¼åˆä¸ªæ€§åŒ– - æ•´ä½“ä¸ªæ€§åŒ–æ•ˆæœ\n\n"
        
        prompt += "ğŸ“° å¾…è¯„ä¼°çš„æ–°é—»æ ‡é¢˜åŠç”¨æˆ·ä¿¡æ¯ï¼š\n"
        
        for i, (title, interest, category, history) in enumerate(zip(titles, interests, categories, histories)):
            prompt += f"\n=== æ ‡é¢˜ {i+1} ===\n"
            prompt += f"æ–°é—»æ ‡é¢˜: \"{title}\"\n"
            prompt += f"æ–°é—»ç±»åˆ«: {category}\n"
            
            primary = interest.get('primary_interest', 'æœªçŸ¥')
            categories_list = interest.get('categories', [])
            prompt += f"ç”¨æˆ·å…´è¶£: ä¸»è¦å…´è¶£={primary}, åå¥½ç±»åˆ«={categories_list}\n"
            
            if history and len(history) > 0:
                recent = history[:3]  # æ˜¾ç¤ºæœ€è¿‘3æ¡
                prompt += f"é˜…è¯»å†å²: {'; '.join(recent)}\n"
            else:
                prompt += "é˜…è¯»å†å²: æ— è®°å½•\n"
        
        prompt += "\nâš ï¸ è¾“å‡ºè¦æ±‚ï¼š\n"
        prompt += "- å¿…é¡»æŒ‰ç…§æ ‡é¢˜é¡ºåºï¼ˆ1åˆ°Nï¼‰ä¾æ¬¡è¯„åˆ†\n"
        prompt += "- æ¯è¡Œè¾“å‡ºä¸€ä¸ªæ ‡é¢˜çš„è¯„åˆ†ï¼Œæ ¼å¼ï¼šç»´åº¦1åˆ†æ•°,ç»´åº¦2åˆ†æ•°,ç»´åº¦3åˆ†æ•°,ç»´åº¦4åˆ†æ•°,ç»´åº¦5åˆ†æ•°\n"
        prompt += "- åªè¾“å‡ºæ•°å­—å’Œé€—å·ï¼Œä¸è¦ä»»ä½•æ–‡å­—è¯´æ˜\n"
        prompt += "- åˆ†æ•°èŒƒå›´ï¼š0-10ï¼ˆå¯ä»¥æ˜¯å°æ•°ï¼Œå¦‚8.5ï¼‰\n\n"
        
        prompt += "ç¤ºä¾‹è¾“å‡ºï¼š\n"
        prompt += "8,7,9,6,8\n"
        prompt += "6,8,5,7,6\n\n"
        
        prompt += "è¯·å¼€å§‹è¯„åˆ†ï¼ˆæŒ‰é¡ºåºå¯¹æ¯ä¸ªæ ‡é¢˜è¾“å‡ºä¸€è¡Œè¯„åˆ†ï¼‰ï¼š\n"
        
        return prompt
    
    def _parse_personalization_evaluation_response(self, response: str) -> List[List[float]]:
        """è§£æLLMä¸ªæ€§åŒ–è¯„ä¼°å“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼åŒ…æ‹¬ç¼ºå°‘é¦–ä¸ªåˆ†æ•°çš„æƒ…å†µ"""
        
        import re
        
        self.logger.debug(f"è§£æä¸ªæ€§åŒ–è¯„ä¼°å“åº”: {response[:200]}...")
        
        try:
            cleaned_text = response.strip()
            score_segments = []
            
            # æ–¹æ³•1ï¼šæŒ‰è¡Œåˆ†å‰²
            lines = [line.strip() for line in cleaned_text.split('\n') if line.strip()]
            
            for line in lines:
                if not line.strip():
                    continue
                
                # è·³è¿‡æ˜æ˜¾çš„è¯´æ˜æ–‡å­—
                if any(keyword in line.lower() for keyword in ['æ ‡é¢˜', 'è¯„ä¼°', 'ç»´åº¦', 'åˆ†æ•°', 'è¾“å‡º', 'ç¤ºä¾‹', 'ä½œä¸º', 'è¯·']):
                    continue
                
                # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—ï¼ˆåŒ…æ‹¬å°æ•°ï¼‰
                numbers = re.findall(r'\d+(?:\.\d+)?', line)
                
                if len(numbers) >= 3:  # è‡³å°‘3ä¸ªæ•°å­—æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„åˆ†æ•°è¡Œ
                    try:
                        scores = [float(num) for num in numbers]
                        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                        scores = [max(0.0, min(10.0, score)) for score in scores]
                        score_segments.append(scores)
                        self.logger.debug(f"ä»è¡Œ '{line}' è§£æå‡ºä¸ªæ€§åŒ–åˆ†æ•°: {scores}")
                    except ValueError:
                        continue
            
            # æ–¹æ³•2ï¼šå¤„ç†è¿ç»­çš„åˆ†æ•°æ®µï¼ˆå¦‚æœæŒ‰è¡Œåˆ†å‰²å¤±è´¥ï¼‰
            if not score_segments:
                # æŒ‰ç©ºæ ¼åˆ†å‰²å¯èƒ½çš„åˆ†æ•°æ®µ
                potential_segments = re.split(r'\s+', cleaned_text)
                
                for segment in potential_segments:
                    segment = segment.strip()
                    if not segment:
                        continue
                    
                    # å¤„ç†ä»¥é€—å·å¼€å¤´çš„åˆ†æ•°æ®µï¼ˆç¼ºå°‘ç¬¬ä¸€ä¸ªåˆ†æ•°ï¼‰
                    if segment.startswith(','):
                        # æ·»åŠ é»˜è®¤åˆ†æ•°5.0
                        segment = '5.0' + segment
                    
                    # æŸ¥æ‰¾åˆ†æ•°
                    numbers = re.findall(r'\d+(?:\.\d+)?', segment)
                    
                    if len(numbers) >= 3:  # è‡³å°‘3ä¸ªæ•°å­—
                        try:
                            scores = [float(num) for num in numbers]
                            scores = [max(0.0, min(10.0, score)) for score in scores]
                            score_segments.append(scores)
                            self.logger.debug(f"ä»æ®µ '{segment}' è§£æå‡ºä¸ªæ€§åŒ–åˆ†æ•°: {scores}")
                        except ValueError:
                            continue
            
            # æ–¹æ³•3ï¼šç›´æ¥è§£ææ‰€æœ‰æ•°å­—
            if not score_segments:
                all_numbers = re.findall(r'\d+(?:\.\d+)?', cleaned_text)
                
                if len(all_numbers) >= 5:  # è‡³å°‘5ä¸ªæ•°å­—
                    try:
                        # å°†æ•°å­—æŒ‰5ä¸ªä¸€ç»„åˆ†ç»„
                        for i in range(0, len(all_numbers), 5):
                            if i + 4 < len(all_numbers):  # ç¡®ä¿æœ‰å®Œæ•´çš„5ä¸ªæ•°å­—
                                group = all_numbers[i:i+5]
                                scores = [float(num) for num in group]
                                scores = [max(0.0, min(10.0, score)) for score in scores]
                                score_segments.append(scores)
                    except ValueError:
                        pass
            
            if not score_segments:
                self.logger.warning("æ²¡æœ‰è§£æå‡ºä»»ä½•ä¸ªæ€§åŒ–åˆ†æ•°ï¼Œè¿”å›é»˜è®¤å€¼")
                return [[5.0, 5.0, 5.0, 5.0, 5.0]]
            
            # ç¡®ä¿æ¯ä¸ªåˆ†æ•°æ®µéƒ½æœ‰5ä¸ªå€¼
            normalized_scores = []
            for scores in score_segments:
                if len(scores) < 5:
                    # ä¸è¶³5ä¸ªåˆ™ç”¨å¹³å‡å€¼è¡¥å……
                    avg_score = np.mean(scores) if scores else 5.0
                    scores.extend([avg_score] * (5 - len(scores)))
                elif len(scores) > 5:
                    # è¶…è¿‡5ä¸ªåˆ™æˆªå–å‰5ä¸ª
                    scores = scores[:5]
                
                normalized_scores.append(scores)
            
            self.logger.info(f"ä¸ªæ€§åŒ–è¯„ä¼°è§£æå®Œæˆï¼Œå…±{len(normalized_scores)}ç»„åˆ†æ•°")
            return normalized_scores
            
        except Exception as e:
            self.logger.error(f"è§£æä¸ªæ€§åŒ–è¯„ä¼°å“åº”å¤±è´¥: {str(e)}")
            return [[5.0, 5.0, 5.0, 5.0, 5.0]]
    
    def llm_evaluate_quality(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¯„ä¼°æ ‡é¢˜è´¨é‡ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        
        self.logger.info("å¼€å§‹LLMè´¨é‡è¯„ä¼°...")
        
        generated_titles = results.get('generated_titles', [])
        reference_titles = results.get('reference_titles', [])
        # ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨news_contentså­—æ®µå
        news_bodies = results.get('news_contents', []) or results.get('news_bodies', [])
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼Œæä¾›æ›´è¯¦ç»†çš„è­¦å‘Šä¿¡æ¯
        if not generated_titles:
            self.logger.warning("ç¼ºå°‘ç”Ÿæˆçš„æ ‡é¢˜æ•°æ®")
            return {
                'llm_quality_score': 0.0,
                'llm_quality_scores': []
            }
        
        if not reference_titles:
            self.logger.warning("ç¼ºå°‘å‚è€ƒæ ‡é¢˜æ•°æ®")
            return {
                'llm_quality_score': 0.0,
                'llm_quality_scores': []
            }
            
        if not news_bodies:
            self.logger.warning("ç¼ºå°‘æ–°é—»å†…å®¹æ•°æ®")
            return {
                'llm_quality_score': 0.0,
                'llm_quality_scores': []
            }
        
        # æ£€æŸ¥æ•°æ®é•¿åº¦ä¸€è‡´æ€§
        if len(generated_titles) != len(reference_titles) or len(generated_titles) != len(news_bodies):
            self.logger.warning(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: ç”Ÿæˆæ ‡é¢˜{len(generated_titles)}, å‚è€ƒæ ‡é¢˜{len(reference_titles)}, æ–°é—»å†…å®¹{len(news_bodies)}")
            # æˆªå–åˆ°æœ€çŸ­é•¿åº¦
            min_length = min(len(generated_titles), len(reference_titles), len(news_bodies))
            generated_titles = generated_titles[:min_length]
            reference_titles = reference_titles[:min_length]
            news_bodies = news_bodies[:min_length]
            self.logger.info(f"å·²è°ƒæ•´æ•°æ®é•¿åº¦ä¸º: {min_length}")
        
        self.logger.info(f"LLMè´¨é‡è¯„ä¼°å‡†å¤‡å°±ç»ªï¼Œå…±{len(generated_titles)}ä¸ªæ ·æœ¬")
        
        # æ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹5ä¸ª
        batch_size = 5
        all_scores = []
        
        for i in range(0, len(generated_titles), batch_size):
            batch_generated = generated_titles[i:i+batch_size]
            batch_reference = reference_titles[i:i+batch_size]
            batch_bodies = news_bodies[i:i+batch_size]
            
            self.logger.info(f"æ­£åœ¨è¯„ä¼°æ‰¹æ¬¡ {i//batch_size + 1}ï¼ŒåŒ…å« {len(batch_generated)} ä¸ªæ ‡é¢˜")
            
            try:
                batch_scores = self._llm_evaluate_batch_quality(
                    batch_generated, batch_reference, batch_bodies
                )
                
                if batch_scores:
                    # batch_scoresæ˜¯äºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨åŒ…å«5ä¸ªç»´åº¦åˆ†æ•°
                    all_scores.extend(batch_scores)
                    self.logger.info(f"æ‰¹æ¬¡è¯„ä¼°æˆåŠŸï¼Œè·å¾— {len(batch_scores)} ä¸ªåˆ†æ•°")
                else:
                    # å¦‚æœæ‰¹æ¬¡è¯„ä¼°å¤±è´¥ï¼Œæ·»åŠ é»˜è®¤åˆ†æ•°
                    default_scores = [[0.5, 0.5, 0.5, 0.5, 0.5] for _ in range(len(batch_generated))]
                    all_scores.extend(default_scores)
                    self.logger.warning(f"æ‰¹æ¬¡è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°")
                    
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡è¯„ä¼°å‡ºé”™: {e}")
                # æ·»åŠ é»˜è®¤åˆ†æ•°
                default_scores = [[0.5, 0.5, 0.5, 0.5, 0.5] for _ in range(len(batch_generated))]
                all_scores.extend(default_scores)
        
        if not all_scores:
            self.logger.warning("æ²¡æœ‰è·å¾—ä»»ä½•LLMè´¨é‡è¯„åˆ†ï¼Œè¿”å›é»˜è®¤å€¼")
            return {
                'llm_quality_score': 0.0,
                'llm_quality_scores': []
            }
        
        # è®¡ç®—æ¯ä¸ªæ ‡é¢˜çš„ç»¼åˆåˆ†æ•°ï¼ˆäº”ä¸ªç»´åº¦çš„å¹³å‡å€¼ï¼‰
        title_scores = []
        for score_list in all_scores:
            if isinstance(score_list, list) and len(score_list) >= 5:
                # å–å‰5ä¸ªç»´åº¦åˆ†æ•°çš„å¹³å‡å€¼
                avg_score = sum(score_list[:5]) / 5.0
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                normalized_score = max(0.0, min(1.0, avg_score))
                title_scores.append(normalized_score)
            else:
                # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                title_scores.append(0.5)
        
        # è®¡ç®—æ€»ä½“å¹³å‡åˆ†
        overall_score = sum(title_scores) / len(title_scores) if title_scores else 0.0
        
        self.logger.info(f"LLMè´¨é‡è¯„ä¼°å®Œæˆï¼Œå¹³å‡åˆ†: {overall_score:.3f}")
        
        return {
            'llm_quality_score': overall_score,
            'llm_quality_scores': title_scores
        }
    
    def _llm_evaluate_batch_quality(self, generated_titles: List[str], 
                                     reference_titles: List[str],
                                     news_bodies: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡LLMè´¨é‡è¯„ä¼°"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»æ ‡é¢˜è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ç”Ÿæˆçš„æ–°é—»æ ‡é¢˜è¿›è¡Œè´¨é‡è¯„åˆ†ã€‚

è¯„ä¼°ç»´åº¦ (æ¯ä¸ªç»´åº¦0-10åˆ†):
1. å‡†ç¡®æ€§: æ ‡é¢˜æ˜¯å¦å‡†ç¡®åæ˜ æ–°é—»å†…å®¹
2. å¸å¼•åŠ›: æ ‡é¢˜æ˜¯å¦èƒ½å¸å¼•è¯»è€…æ³¨æ„
3. æ¸…æ™°åº¦: æ ‡é¢˜è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. ç®€æ´æ€§: æ ‡é¢˜é•¿åº¦å’Œè¡¨è¾¾æ˜¯å¦ç®€æ´
5. ç»¼åˆè´¨é‡: æ•´ä½“æ ‡é¢˜è´¨é‡

ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºï¼š
- åªè¾“å‡ºæ•°å­—å’Œé€—å·ï¼Œä¸è¦ä»»ä½•æ–‡å­—è¯´æ˜
- æ¯è¡Œä¸€ä¸ªæ ‡é¢˜çš„è¯„åˆ†ï¼Œæ ¼å¼ï¼šåˆ†æ•°1,åˆ†æ•°2,åˆ†æ•°3,åˆ†æ•°4,åˆ†æ•°5
- åˆ†æ•°èŒƒå›´0-10ï¼Œå¯ä»¥æ˜¯å°æ•°ï¼ˆå¦‚8.5ï¼‰
- æŒ‰ç…§æ ‡é¢˜é¡ºåºä¾æ¬¡è¾“å‡º
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€åˆ†ææˆ–å…¶ä»–æ–‡å­—

ç¤ºä¾‹è¾“å‡ºï¼š
8.5,7.0,9.0,6.5,8.0
6.0,8.5,5.5,7.0,6.5"""
        
        user_prompt = "è¯·å¯¹ä»¥ä¸‹æ–°é—»æ ‡é¢˜è¿›è¡Œè´¨é‡è¯„ä¼°ï¼š\n\n"
        
        for i, (gen, orig, content) in enumerate(zip(generated_titles, reference_titles, news_bodies)):
            user_prompt += f"=== æ ‡é¢˜ {i+1} ===\n"
            user_prompt += f"åŸæ ‡é¢˜: {orig}\n"
            user_prompt += f"ç”Ÿæˆæ ‡é¢˜: {gen}\n"
            user_prompt += f"æ–°é—»å†…å®¹: {content[:200]}...\n\n"
        
        user_prompt += "è¯·æŒ‰ç…§æ ‡é¢˜é¡ºåºï¼Œæ¯è¡Œè¾“å‡ºä¸€ä¸ªæ ‡é¢˜çš„è¯„åˆ†ï¼ˆ5ä¸ªç»´åº¦åˆ†æ•°ï¼Œç”¨é€—å·åˆ†éš”ï¼‰ï¼š"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            response = self.llm_client.chat_completion(messages, max_tokens=1000)
            if response:
                batch_scores = self._parse_llm_evaluation_response(response, len(generated_titles))
                if batch_scores:  # ç¡®ä¿è§£ææˆåŠŸ
                    return batch_scores
                else:
                    # è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                    return [[0.5, 0.5, 0.5, 0.5, 0.5]] * len(generated_titles)
            else:
                # APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                return [[0.5, 0.5, 0.5, 0.5, 0.5]] * len(generated_titles)
            
        except Exception as e:
            self.logger.error(f"LLMè´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æ•°çŸ©é˜µ
            return [[0.5, 0.5, 0.5, 0.5, 0.5]] * len(generated_titles)
    
    def _parse_llm_evaluation_response(self, response_text: str, expected_count: int) -> List[List[float]]:
        """è§£æLLMè¯„ä¼°å“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œç‰¹åˆ«å¤„ç†ç¼ºå°‘é¦–ä¸ªåˆ†æ•°çš„æƒ…å†µ"""
        
        self.logger.debug(f"è§£æLLMå“åº”: {response_text[:200]}...")
        
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_text = response_text.strip()
            
            # å¤„ç†ç¼ºå°‘ç¬¬ä¸€ä¸ªåˆ†æ•°çš„æƒ…å†µï¼Œå¦‚ï¼š",8.0,6.0,5.5,6.8 6.0,6.5,5.0,7.0,6.1"
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†æ•°æ®µ
            import re
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†æ•°æ¨¡å¼
            # åŒ…æ‹¬ï¼šé€—å·åˆ†éš”ã€ç©ºæ ¼åˆ†éš”ã€ä»¥é€—å·å¼€å¤´çš„åˆ†æ•°æ®µ
            score_segments = []
            
            # æ–¹æ³•1ï¼šæŒ‰è¡Œåˆ†å‰²
            lines = [line.strip() for line in cleaned_text.split('\n') if line.strip()]
            
            for line in lines:
                if not line.strip():
                    continue
                
                # è·³è¿‡æ˜æ˜¾çš„è¯´æ˜æ–‡å­—
                if any(keyword in line.lower() for keyword in ['æ ‡é¢˜', 'è¯„ä¼°', 'ç»´åº¦', 'åˆ†æ•°', 'è¾“å‡º', 'ç¤ºä¾‹']):
                    continue
                
                # å¤„ç†å¯èƒ½çš„åˆ†æ•°è¡Œ
                # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—ï¼ˆåŒ…æ‹¬å°æ•°ï¼‰
                numbers = re.findall(r'\d+(?:\.\d+)?', line)
                
                if len(numbers) >= 3:  # è‡³å°‘3ä¸ªæ•°å­—æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„åˆ†æ•°è¡Œ
                    try:
                        scores = [float(num) for num in numbers]
                        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                        scores = [max(0.0, min(10.0, score)) for score in scores]
                        score_segments.append(scores)
                        self.logger.debug(f"ä»è¡Œ '{line}' è§£æå‡ºåˆ†æ•°: {scores}")
                    except ValueError:
                        continue
            
            # æ–¹æ³•2ï¼šå¤„ç†è¿ç»­çš„åˆ†æ•°æ®µï¼ˆå¦‚æœæŒ‰è¡Œåˆ†å‰²å¤±è´¥ï¼‰
            if not score_segments:
                # å°è¯•æŸ¥æ‰¾è¿ç»­çš„æ•°å­—åˆ†æ•°æ®µ
                # åŒ¹é…ç±»ä¼¼ ",8.0,6.0,5.5,6.8 6.0,6.5,5.0,7.0,6.1" çš„æ ¼å¼
                
                # é¦–å…ˆæŒ‰ç©ºæ ¼æˆ–æ¢è¡Œåˆ†å‰²å¯èƒ½çš„åˆ†æ•°æ®µ
                potential_segments = re.split(r'\s+', cleaned_text)
                
                for segment in potential_segments:
                    segment = segment.strip()
                    if not segment:
                        continue
                    
                    # å¤„ç†ä»¥é€—å·å¼€å¤´çš„åˆ†æ•°æ®µï¼ˆç¼ºå°‘ç¬¬ä¸€ä¸ªåˆ†æ•°ï¼‰
                    if segment.startswith(','):
                        # ç§»é™¤å¼€å¤´çš„é€—å·ï¼Œæ·»åŠ é»˜è®¤åˆ†æ•°
                        segment = '5.0' + segment  # æ·»åŠ é»˜è®¤åˆ†æ•°5.0
                    
                    # æŸ¥æ‰¾åˆ†æ•°
                    numbers = re.findall(r'\d+(?:\.\d+)?', segment)
                    
                    if len(numbers) >= 3:  # è‡³å°‘3ä¸ªæ•°å­—
                        try:
                            scores = [float(num) for num in numbers]
                            scores = [max(0.0, min(10.0, score)) for score in scores]
                            score_segments.append(scores)
                            self.logger.debug(f"ä»æ®µ '{segment}' è§£æå‡ºåˆ†æ•°: {scores}")
                        except ValueError:
                            continue
            
            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
            if not score_segments:
                # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
                all_numbers = re.findall(r'\d+(?:\.\d+)?', cleaned_text)
                
                if len(all_numbers) >= 5:  # è‡³å°‘5ä¸ªæ•°å­—
                    try:
                        # å°†æ•°å­—æŒ‰5ä¸ªä¸€ç»„åˆ†ç»„
                        for i in range(0, len(all_numbers), 5):
                            if i + 4 < len(all_numbers):  # ç¡®ä¿æœ‰å®Œæ•´çš„5ä¸ªæ•°å­—
                                group = all_numbers[i:i+5]
                                scores = [float(num) for num in group]
                                scores = [max(0.0, min(10.0, score)) for score in scores]
                                score_segments.append(scores)
                    except ValueError:
                        pass
            
            if not score_segments:
                self.logger.warning("æ²¡æœ‰è§£æå‡ºä»»ä½•åˆ†æ•°ï¼Œè¿”å›é»˜è®¤å€¼")
                return [[5.0, 5.0, 5.0, 5.0, 5.0]] * expected_count
            
            # ç¡®ä¿æ¯ä¸ªåˆ†æ•°æ®µéƒ½æœ‰5ä¸ªå€¼
            normalized_scores = []
            for scores in score_segments:
                if len(scores) < 5:
                    # ä¸è¶³5ä¸ªåˆ™ç”¨å¹³å‡å€¼è¡¥å……
                    avg_score = np.mean(scores) if scores else 5.0
                    scores.extend([avg_score] * (5 - len(scores)))
                elif len(scores) > 5:
                    # è¶…è¿‡5ä¸ªåˆ™æˆªå–å‰5ä¸ª
                    scores = scores[:5]
                
                # è½¬æ¢ä¸º0-1èŒƒå›´
                normalized = [score / 10.0 for score in scores]
                normalized = [max(0.0, min(1.0, score)) for score in normalized]
                normalized_scores.append(normalized)
            
            # è°ƒæ•´æ•°é‡ä»¥åŒ¹é…æœŸæœ›å€¼
            if len(normalized_scores) > expected_count:
                normalized_scores = normalized_scores[:expected_count]
            elif len(normalized_scores) < expected_count:
                # ä¸è¶³åˆ™å¤åˆ¶æœ€åä¸€ç»„
                last_scores = normalized_scores[-1] if normalized_scores else [0.5, 0.5, 0.5, 0.5, 0.5]
                while len(normalized_scores) < expected_count:
                    normalized_scores.append(last_scores.copy())
            
            self.logger.info(f"è§£æå®Œæˆï¼Œå…±{len(normalized_scores)}ç»„åˆ†æ•°ï¼Œæ¯ç»„{len(normalized_scores[0])}ä¸ªå€¼")
            return normalized_scores
            
        except Exception as e:
            self.logger.error(f"è§£æLLMè¯„ä¼°å“åº”å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤åˆ†æ•°çŸ©é˜µ
            return [[0.5, 0.5, 0.5, 0.5, 0.5]] * expected_count
    
    def comprehensive_evaluation(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆè¯„ä¼°"""
        
        eval_results = {}
        
        # åŸºç¡€ç»Ÿè®¡
        generated_titles = results.get('generated_titles', [])
        reference_titles = results.get('reference_titles', [])
        user_interests = results.get('user_interests', [])
        news_categories = results.get('news_categories', [])
        user_histories = results.get('user_histories', [])
        original_titles = results.get('original_titles', [])
        news_contents = results.get('news_contents', [])
        
        eval_results['basic_stats'] = self._calculate_basic_stats(generated_titles, reference_titles)
        
        # ROUGEè¯„ä¼°
        if generated_titles and reference_titles:
            eval_results['rouge_scores'] = self.calculate_rouge_scores(generated_titles, reference_titles)
        
        # ä¸ªæ€§åŒ–è¯„ä¼°ï¼ˆè§„åˆ™åŸºç¡€ï¼‰- æš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºæ•ˆæœä¸ä½³
        # if generated_titles and user_interests and news_categories:
        #     eval_results['rule_based_personalization'] = self.evaluate_personalization(
        #         generated_titles, user_interests, news_categories, user_histories)
        
        # ä¸ªæ€§åŒ–è¯„ä¼°ï¼ˆLLMï¼‰
        if self.use_llm_evaluation and generated_titles and user_interests and news_categories:
            self.logger.info("å¼€å§‹LLMä¸ªæ€§åŒ–è¯„ä¼°...")
            eval_results['llm_personalization'] = self.llm_evaluate_personalization(
                generated_titles, user_interests, news_categories, user_histories)
        
        # æ ‡é¢˜è´¨é‡è¯„ä¼°
        eval_results['title_quality'] = self._evaluate_title_quality(generated_titles)
        
        # LLMè´¨é‡è¯„ä¼°
        if self.use_llm_evaluation and generated_titles and original_titles and news_contents:
            self.logger.info("å¼€å§‹LLMè´¨é‡è¯„ä¼°...")
            eval_results['llm_evaluation'] = self.llm_evaluate_quality(results)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        comprehensive_scores = self._calculate_comprehensive_score(eval_results)
        eval_results['comprehensive_scores'] = comprehensive_scores
        eval_results['overall_score'] = comprehensive_scores.get('final_comprehensive_score', 0.0)
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°å®ä¾‹å±æ€§
        self.evaluation_results = eval_results
        
        return eval_results
    
    def _calculate_basic_stats(self, generated_titles: List[str], reference_titles: List[str]) -> Dict[str, Any]:
        """è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        
        stats = {
            'total_samples': len(generated_titles),
            'valid_samples': len([t for t in generated_titles if t and t.strip()]),
            'success_rate': 0.0,
            'avg_generated_length': 0.0,
            'avg_reference_length': 0.0
        }
        
        if stats['total_samples'] > 0:
            stats['success_rate'] = stats['valid_samples'] / stats['total_samples']
            
            valid_generated = [t for t in generated_titles if t and t.strip()]
            if valid_generated:
                stats['avg_generated_length'] = np.mean([len(t) for t in valid_generated])
            
            valid_reference = [t for t in reference_titles if t and t.strip()]
            if valid_reference:
                stats['avg_reference_length'] = np.mean([len(t) for t in valid_reference])
        
        return stats
    
    def _evaluate_title_quality(self, generated_titles: List[str]) -> Dict[str, float]:
        """è¯„ä¼°æ ‡é¢˜è´¨é‡ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        
        if not generated_titles:
            return {}
        
        length_scores = []
        diversity_score = 0.0
        
        valid_titles = [t for t in generated_titles if t and t.strip()]
        
        if not valid_titles:
            return {}
        
        # é•¿åº¦åˆç†æ€§è¯„ä¼°
        for title in valid_titles:
            length = len(title)
            if 20 <= length <= 80:  # ç†æƒ³é•¿åº¦èŒƒå›´
                length_scores.append(1.0)
            elif 10 <= length <= 100:  # å¯æ¥å—èŒƒå›´
                length_scores.append(0.7)
            else:
                length_scores.append(0.3)
        
        # å¤šæ ·æ€§è¯„ä¼°
        unique_titles = set(valid_titles)
        diversity_score = len(unique_titles) / len(valid_titles)
        
        return {
            'length_reasonableness': np.mean(length_scores),
            'title_diversity': diversity_score,
            'average_length': np.mean([len(t) for t in valid_titles])
        }
    
    def _calculate_comprehensive_score(self, eval_results: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåˆ†åˆ«è®¡ç®—ROUGEå’ŒLLMç»¼åˆå¾—åˆ†ï¼Œå¹¶åŠ æƒï¼‰"""
        
        scores_breakdown = {
            'rouge_score': 0.0,
            'llm_quality_score': 0.0,
            'llm_personalization_score': 0.0,
            'rule_personalization_score': 0.0,
            'title_quality_score': 0.0
        }
        
        # 1. ROUGEè¯„åˆ†
        rouge_scores = eval_results.get('rouge_scores', {})
        if rouge_scores:
            rouge_avg = np.mean([
                rouge_scores.get('rouge1_f', 0),
                rouge_scores.get('rouge2_f', 0),
                rouge_scores.get('rougeL_f', 0)
            ])
            scores_breakdown['rouge_score'] = rouge_avg
        
        # 2. LLMè´¨é‡è¯„åˆ†ï¼ˆå·²ç»æ˜¯0-1èŒƒå›´ï¼‰
        llm_eval = eval_results.get('llm_evaluation', {})
        if llm_eval:
            llm_quality_score = llm_eval.get('llm_quality_score', 0)
            scores_breakdown['llm_quality_score'] = llm_quality_score  # å·²ç»æ˜¯0-1èŒƒå›´ï¼Œæ— éœ€è½¬æ¢
        
        # 3. LLMä¸ªæ€§åŒ–è¯„åˆ†
        llm_personalization = eval_results.get('llm_personalization', {})
        if llm_personalization:
            llm_personal_score = llm_personalization.get('llm_overall_personalization', 0)
            scores_breakdown['llm_personalization_score'] = llm_personal_score
        
        # 4. æ ‡é¢˜è´¨é‡è¯„åˆ†
        title_quality = eval_results.get('title_quality', {})
        if title_quality:
            quality_avg = np.mean([
                title_quality.get('length_reasonableness', 0),
                title_quality.get('title_diversity', 0)
            ])
            scores_breakdown['title_quality_score'] = quality_avg
        
        # è®¡ç®—åŠ æƒç»¼åˆåˆ†æ•°ï¼ˆç§»é™¤è§„åˆ™ä¸ªæ€§åŒ–è¯„ä¼°åé‡æ–°åˆ†é…æƒé‡ï¼‰
        # ROUGEæƒé‡35%, LLMè´¨é‡æƒé‡30%, LLMä¸ªæ€§åŒ–æƒé‡25%, æ ‡é¢˜è´¨é‡æƒé‡10%
        final_score = (
            scores_breakdown['rouge_score'] * 0.35 +
            scores_breakdown['llm_quality_score'] * 0.30 +
            scores_breakdown['llm_personalization_score'] * 0.25 +
            scores_breakdown['title_quality_score'] * 0.10
        )
        
        # ç¡®ä¿æœ€ç»ˆåˆ†æ•°åœ¨0-1èŒƒå›´å†…
        final_score = min(max(final_score, 0.0), 1.0)
        scores_breakdown['final_comprehensive_score'] = final_score
        
        return scores_breakdown
    
    def generate_evaluation_report(self, eval_results: Dict[str, Any], save_path: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Šï¼ˆç»Ÿä¸€æƒé‡ä½“ç³»ï¼‰"""
        
        report = "=" * 80 + "\n"
        report += "ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆ - è¯¦ç»†è¯„ä¼°æŠ¥å‘Š\n"
        report += "=" * 80 + "\n\n"
        
        # åŸºæœ¬ç»Ÿè®¡
        basic_stats = eval_results.get('basic_stats', {})
        report += "ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:\n"
        report += f"â”œâ”€ æ€»æ ·æœ¬æ•°: {basic_stats.get('total_samples', 0)}\n"
        report += f"â”œâ”€ æœ‰æ•ˆæ ·æœ¬æ•°: {basic_stats.get('valid_samples', 0)}\n"
        report += f"â””â”€ æˆåŠŸç‡: {basic_stats.get('success_rate', 0):.2%}\n\n"
        
        # ROUGEè¯„åˆ†ï¼ˆè‡ªåŠ¨è¯„ä¼°ï¼‰
        rouge_scores = eval_results.get('rouge_scores', {})
        if rouge_scores:
            report += "ğŸ“ ROUGEè¯„åˆ† (è‡ªåŠ¨è¯„ä¼° - ä¸å‚è€ƒæ ‡é¢˜çš„ç›¸ä¼¼æ€§):\n"
            report += f"â”œâ”€ ROUGE-1 F-Score: {rouge_scores.get('rouge1_f', 0):.4f}\n"
            report += f"â”œâ”€ ROUGE-2 F-Score: {rouge_scores.get('rouge2_f', 0):.4f}\n"
            report += f"â””â”€ ROUGE-L F-Score: {rouge_scores.get('rougeL_f', 0):.4f}\n\n"
        
        # è°ƒè¯•ä¿¡æ¯ï¼šLLMè´¨é‡è¯„ä¼°
        llm_eval = eval_results.get('llm_evaluation', {})
        if llm_eval:
            report += "ğŸ¤– LLMè´¨é‡è¯„ä¼°è¯¦æƒ…:\n"
            llm_quality_score = llm_eval.get('llm_quality_score', 0)
            report += f"â”œâ”€ è´¨é‡å¾—åˆ† (0-1): {llm_quality_score:.4f}\n"
            report += f"â”œâ”€ ç­‰æ•ˆ10åˆ†åˆ¶: {llm_quality_score * 10:.2f}/10\n"
            report += f"â””â”€ è¯„åˆ†æ ‡å‡†å·®: {llm_eval.get('llm_quality_std', 0):.4f}\n\n"
        else:
            report += "âš ï¸ è­¦å‘Š: LLMè´¨é‡è¯„ä¼°æœªè¿è¡Œæˆ–å¤±è´¥\n\n"
        
        # LLMä¸ªæ€§åŒ–è¯„ä¼°
        llm_personalization = eval_results.get('llm_personalization', {})
        if llm_personalization:
            report += "ğŸ¯ LLMä¸ªæ€§åŒ–è¯„ä¼° (å¤§æ¨¡å‹è¯„åˆ† - ä¸ªæ€§åŒ–ç¨‹åº¦):\n"
            report += f"â”œâ”€ å…´è¶£åŒ¹é…åº¦: {llm_personalization.get('llm_interest_match', 0):.4f}\n"
            report += f"â”œâ”€ ç±»åˆ«ç›¸å…³æ€§: {llm_personalization.get('llm_category_relevance', 0):.4f}\n"
            report += f"â”œâ”€ å†å²ä¸€è‡´æ€§: {llm_personalization.get('llm_history_consistency', 0):.4f}\n"
            report += f"â”œâ”€ ä¸ªæ€§åŒ–åˆ›æ–°: {llm_personalization.get('llm_personalization_innovation', 0):.4f}\n"
            report += f"â””â”€ LLMç»¼åˆä¸ªæ€§åŒ–: {llm_personalization.get('llm_overall_personalization', 0):.4f}\n\n"
        
        # æ ‡é¢˜è´¨é‡ï¼ˆè§„åˆ™è¯„ä¼°ï¼‰
        title_quality = eval_results.get('title_quality', {})
        if title_quality:
            report += "âœ¨ æ ‡é¢˜è´¨é‡è¯„ä¼° (è‡ªåŠ¨è¯„ä¼° - åŸºäºè§„åˆ™):\n"
            report += f"â”œâ”€ é•¿åº¦åˆç†æ€§: {title_quality.get('length_reasonableness', 0):.4f}\n"
            report += f"â”œâ”€ æ ‡é¢˜å¤šæ ·æ€§: {title_quality.get('title_diversity', 0):.4f}\n"
            report += f"â””â”€ å¹³å‡æ ‡é¢˜é•¿åº¦: {title_quality.get('average_length', 0):.1f} å­—ç¬¦\n\n"
        
        # ç»¼åˆè¯„åˆ†è¯¦ç»†å±•ç¤ºï¼ˆç»Ÿä¸€æƒé‡ä½“ç³»ï¼‰
        comprehensive_scores = eval_results.get('comprehensive_scores', {})
        if comprehensive_scores:
            report += "ğŸ† ç»¼åˆè¯„åˆ†è¯¦æƒ… (åŠ æƒè®¡ç®—):\n"
            report += f"â”œâ”€ ROUGEå¾—åˆ† (æƒé‡35%): {comprehensive_scores.get('rouge_score', 0):.4f}\n"
            report += f"â”œâ”€ LLMè´¨é‡å¾—åˆ† (æƒé‡30%): {comprehensive_scores.get('llm_quality_score', 0):.4f}\n"
            report += f"â”œâ”€ LLMä¸ªæ€§åŒ–å¾—åˆ† (æƒé‡25%): {comprehensive_scores.get('llm_personalization_score', 0):.4f}\n"
            report += f"â”œâ”€ æ ‡é¢˜è´¨é‡å¾—åˆ† (æƒé‡10%): {comprehensive_scores.get('title_quality_score', 0):.4f}\n"
            report += f"â””â”€ ğŸ“ˆ æœ€ç»ˆç»¼åˆå¾—åˆ†: {comprehensive_scores.get('final_comprehensive_score', 0):.4f}\n\n"
        else:
            overall_score = eval_results.get('overall_score', 0)
            if isinstance(overall_score, dict):
                final_score = overall_score.get('final_comprehensive_score', 0)
            else:
                final_score = overall_score
            report += f"ğŸ† ç»¼åˆè¯„åˆ†: {final_score:.4f}\n\n"
        
        # è¯„åˆ†è¯´æ˜
        report += "ğŸ“‹ è¯„åˆ†è¯´æ˜:\n"
        report += "â€¢ ROUGEè¯„åˆ† (35%): è¡¡é‡ç”Ÿæˆæ ‡é¢˜ä¸å‚è€ƒæ ‡é¢˜çš„è¯æ±‡é‡å åº¦\n"
        report += "â€¢ LLMè´¨é‡è¯„åˆ† (30%): å¤§æ¨¡å‹ä»å‡†ç¡®æ€§ã€å¸å¼•åŠ›ã€æ¸…æ™°åº¦ç­‰ç»´åº¦è¯„åˆ†\n"
        report += "â€¢ LLMä¸ªæ€§åŒ–è¯„åˆ† (25%): å¤§æ¨¡å‹ä»ä¸ªæ€§åŒ–è§’åº¦è¯„ä¼°æ ‡é¢˜è´¨é‡\n"
        report += "â€¢ æ ‡é¢˜è´¨é‡è¯„åˆ† (10%): åŸºäºé•¿åº¦åˆç†æ€§ã€å¤šæ ·æ€§ç­‰è§„åˆ™è¯„ä¼°\n"
        report += "â€¢ æœ€ç»ˆå¾—åˆ†: å„é¡¹è¯„åˆ†çš„åŠ æƒå¹³å‡ï¼ˆèŒƒå›´0-1ï¼‰\n\n"
        
        # è¯„çº§
        final_score = 0.0
        if comprehensive_scores:
            final_score = comprehensive_scores.get('final_comprehensive_score', 0)
        else:
            overall_score = eval_results.get('overall_score', 0)
            if isinstance(overall_score, dict):
                final_score = overall_score.get('final_comprehensive_score', 0)
            else:
                final_score = overall_score
        
        report += "ğŸ¯ è¯´æ˜: æœ¬è¯„ä¼°é›†æˆäº†ROUGEè‡ªåŠ¨è¯„ä¼°å’ŒLLMæ™ºèƒ½è¯„ä¼°ï¼Œå…¨é¢åæ˜ æ ‡é¢˜ç”Ÿæˆè´¨é‡\n"
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            self.logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report
    
    def save_detailed_results(self, save_path: str):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        
        if not hasattr(self, 'evaluation_results') or not self.evaluation_results:
            self.logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœå¯ä¿å­˜")
            return False
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # å‡†å¤‡ä¿å­˜æ•°æ®
            save_data = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'evaluation_results': self.evaluation_results
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def create_comparison_chart(self, save_path: Optional[str] = None):
        """åˆ›å»ºè¯„ä¼°ç»“æœå¯¹æ¯”å›¾è¡¨"""
        
        if not hasattr(self, 'evaluation_results') or not self.evaluation_results:
            self.logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœå¯ç”¨äºç”Ÿæˆå›¾è¡¨")
            return False
        
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            eval_results = self.evaluation_results
            
            # åˆ›å»º2x2çš„å­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle('ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆ - è¯„ä¼°ç»“æœ', fontsize=16)
            
            # 1. ROUGEåˆ†æ•°
            rouge_scores = eval_results.get('rouge_scores', {})
            if rouge_scores:
                rouge_metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']
                rouge_values = [
                    rouge_scores.get('rouge1_f', 0),
                    rouge_scores.get('rouge2_f', 0),
                    rouge_scores.get('rougeL_f', 0)
                ]
                
                axes[0, 0].bar(rouge_metrics, rouge_values, color='skyblue', alpha=0.8)
                axes[0, 0].set_title('ROUGE F-Scores')
                axes[0, 0].set_ylabel('Score')
                axes[0, 0].set_ylim(0, 1)
                for i, v in enumerate(rouge_values):
                    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            
            # 2. LLMè¯„ä¼°ï¼ˆè´¨é‡+ä¸ªæ€§åŒ–ï¼‰
            llm_eval = eval_results.get('llm_evaluation', {})
            llm_personalization = eval_results.get('llm_personalization', {})
            if llm_eval or llm_personalization:
                llm_labels = ['è´¨é‡è¯„åˆ†', 'ä¸ªæ€§åŒ–è¯„åˆ†']
                llm_values = [
                    llm_eval.get('llm_quality_score', 0),
                    llm_personalization.get('llm_overall_personalization', 0)
                ]
                
                axes[0, 1].bar(llm_labels, llm_values, color='lightcoral', alpha=0.8)
                axes[0, 1].set_title('LLMè¯„ä¼°')
                axes[0, 1].set_ylabel('Score')
                axes[0, 1].set_ylim(0, 1)
                for i, v in enumerate(llm_values):
                    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            else:
                axes[0, 1].text(0.5, 0.5, 'LLMè¯„ä¼°æ•°æ®\nä¸å¯ç”¨', ha='center', va='center', 
                               transform=axes[0, 1].transAxes, fontsize=12)
                axes[0, 1].set_title('LLMè¯„ä¼°')
            
            # 3. æ ‡é¢˜è´¨é‡
            title_quality = eval_results.get('title_quality', {})
            if title_quality:
                quality_labels = ['é•¿åº¦\nåˆç†æ€§', 'æ ‡é¢˜\nå¤šæ ·æ€§']
                quality_values = [
                    title_quality.get('length_reasonableness', 0),
                    title_quality.get('title_diversity', 0)
                ]
                
                axes[1, 0].bar(quality_labels, quality_values, color='lightgreen', alpha=0.8)
                axes[1, 0].set_title('æ ‡é¢˜è´¨é‡')
                axes[1, 0].set_ylabel('Score')
                axes[1, 0].set_ylim(0, 1)
                for i, v in enumerate(quality_values):
                    axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
            else:
                axes[1, 0].text(0.5, 0.5, 'æ ‡é¢˜è´¨é‡æ•°æ®\nä¸å¯ç”¨', ha='center', va='center', 
                               transform=axes[1, 0].transAxes, fontsize=12)
                axes[1, 0].set_title('æ ‡é¢˜è´¨é‡')
            
            # 4. ç»¼åˆè¯„åˆ†é¥¼å›¾ï¼ˆä¿®å¤è´Ÿå€¼é—®é¢˜ï¼‰
            comprehensive_scores = eval_results.get('comprehensive_scores', {})
            if comprehensive_scores:
                overall_score = comprehensive_scores.get('final_comprehensive_score', 0)
            else:
                overall_score = eval_results.get('overall_score', 0)
                if isinstance(overall_score, dict):
                    overall_score = overall_score.get('final_comprehensive_score', 0)
            
            # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
            overall_score = max(0.0, min(1.0, overall_score))
            remaining_score = 1.0 - overall_score
            
            # ç¡®ä¿ä¸¤ä¸ªå€¼éƒ½ä¸ä¸ºè´Ÿ
            if overall_score < 0 or remaining_score < 0:
                overall_score = 0.5
                remaining_score = 0.5
            
            colors = ['gold', 'lightgray']
            wedge_sizes = [overall_score, remaining_score]
            
            axes[1, 1].pie(wedge_sizes, 
                           labels=['å·²è¾¾æˆ', 'å¾…æå‡'], 
                           colors=colors,
                           autopct='%1.1f%%',
                           startangle=90)
            axes[1, 1].set_title(f'ç»¼åˆè¯„åˆ†: {overall_score:.3f}')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            if save_path:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"è¯„ä¼°å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
            
            plt.close()
            return True
            
        except ImportError as e:
            self.logger.warning(f"æ— æ³•ç”Ÿæˆå›¾è¡¨ï¼Œç¼ºå°‘matplotlib: {str(e)}")
            return False
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°å›¾è¡¨å¤±è´¥: {str(e)}")
            return False

if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°å™¨
    evaluator = Evaluator()
    
    # æ¨¡æ‹Ÿæ•°æ®
    test_results = {
        'generated_titles': [
            'ç§‘æŠ€å·¨å¤´æ¨å‡ºAIæ–°å“',
            'æ™ºèƒ½æ‰‹æœºæ€§èƒ½å¤§å‡çº§',
            'æ–°æŠ€æœ¯å¼•é¢†è¡Œä¸šå˜é©'
        ],
        'reference_titles': [
            'ç§‘æŠ€å…¬å¸å‘å¸ƒæ–°äº§å“',
            'æ‰‹æœºå‚å•†æ¨å‡ºæ——èˆ°æœº',
            'æŠ€æœ¯åˆ›æ–°æ¨åŠ¨å‘å±•'
        ],
        'user_interests': [
            {'primary_interest': 'Technology', 'categories': ['AI', 'Mobile']},
            {'primary_interest': 'Technology', 'categories': ['Hardware']},
            {'primary_interest': 'Business', 'categories': ['Innovation']}
        ],
        'news_categories': ['Technology', 'Technology', 'Business']
    }
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.comprehensive_evaluation(test_results)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_evaluation_report(results)
    print(report)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_detailed_results('./test_evaluation_results.json') 