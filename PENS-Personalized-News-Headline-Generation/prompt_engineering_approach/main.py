"""
ä¸»ç¨‹åº - åŸºäºæç¤ºè¯å·¥ç¨‹çš„ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆ
"""

import os
import json
import logging
import argparse
from datetime import datetime
from tqdm import tqdm
from typing import List, Dict, Any

from config import ensure_directories, DATA_CONFIG, PROMPT_CONFIG, API_CONFIG, DATA_PATHS
from data_processor import DataProcessor
from llm_client import LLMClient
from prompt_generator import PromptGenerator
from evaluator import Evaluator

class PersonalizedTitleGenerator:
    """ä¸ªæ€§åŒ–æ ‡é¢˜ç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, config_override: Dict = None):
        self.logger = self._setup_logger()
        
        # åº”ç”¨é…ç½®è¦†ç›–
        if config_override:
            self.config = config_override
        else:
            self.config = {
                'data': DATA_CONFIG,
                'prompt': PROMPT_CONFIG,
                'api': API_CONFIG,
                'paths': DATA_PATHS
            }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = DataProcessor()
        self.llm_client = LLMClient()
        self.prompt_generator = PromptGenerator()
        self.evaluator = Evaluator()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'generated_titles': [],
            'reference_titles': [],
            'user_interests': [],
            'news_categories': [],
            'generation_metadata': []
        }
        
        ensure_directories()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def load_or_process_data(self, force_reprocess: bool = False) -> bool:
        """åŠ è½½æˆ–å¤„ç†æ•°æ®"""
        
        processed_data_path = os.path.join(self.config['paths']['processed_data_dir'], 'test_samples.json')
        
        if not force_reprocess and os.path.exists(processed_data_path):
            self.logger.info("æ£€æµ‹åˆ°å·²å¤„ç†çš„æ•°æ®ï¼Œç›´æ¥åŠ è½½...")
            success = self.data_processor.load_processed_data()
            if success:
                return True
            else:
                self.logger.warning("åŠ è½½å·²å¤„ç†æ•°æ®å¤±è´¥ï¼Œé‡æ–°å¤„ç†...")
        
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        success = self.data_processor.process_all_from_tsv()
        
        if not success:
            self.logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            return False
        
        self.logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå…± {len(self.data_processor.test_samples)} ä¸ªæ ·æœ¬")
        return True
    
    def generate_titles_single(self, samples: List[Dict[str, Any]], 
                              start_idx: int = 0, end_idx: int = None) -> List[str]:
        """å•ä¸ªæ ·æœ¬é€ä¸€ç”Ÿæˆæ ‡é¢˜"""
        
        if end_idx is None:
            end_idx = len(samples)
        
        generated_titles = []
        
        self.logger.info(f"å¼€å§‹å•ä¸ªç”Ÿæˆæ¨¡å¼ï¼Œå¤„ç†æ ·æœ¬ {start_idx} åˆ° {end_idx}")
        
        for i, sample in enumerate(tqdm(samples[start_idx:end_idx], desc="ç”Ÿæˆæ ‡é¢˜")):
            try:
                # ç”Ÿæˆä¸ªæ€§åŒ–æç¤ºè¯
                system_prompt, user_prompt = self.prompt_generator.generate_single_prompt(
                    sample, 
                    style='enhanced'
                )
                
                # è°ƒç”¨LLMç”Ÿæˆæ ‡é¢˜
                response = self.llm_client.generate_personalized_title(
                    sample, 
                    system_prompt, 
                    user_prompt
                )
                
                if response:
                    # è§£æå¤šä¸ªæ ‡é¢˜é€‰é¡¹ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                    title_options = self.llm_client.parse_multiple_titles(response)
                    if title_options:
                        generated_title = title_options[0]
                    else:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”çš„ç¬¬ä¸€è¡Œ
                        generated_title = response.split('\n')[0].strip()
                        generated_title = generated_title.replace('æ ‡é¢˜:', '').replace(':', '').strip()
                    
                    generated_titles.append(generated_title)
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    self.results['generation_metadata'].append({
                        'sample_idx': start_idx + i,
                        'prompt_style': 'enhanced',
                        'response_raw': response,
                        'title_options': title_options if 'title_options' in locals() else [generated_title],
                        'selected_title': generated_title
                    })
                else:
                    generated_titles.append("ç”Ÿæˆå¤±è´¥")
                    self.results['generation_metadata'].append({
                        'sample_idx': start_idx + i,
                        'error': 'APIè°ƒç”¨å¤±è´¥'
                    })
                
                # æ£€æŸ¥APIä½¿ç”¨æƒ…å†µ
                usage_stats = self.llm_client.get_usage_stats()
                if usage_stats['remaining_tokens'] <= 1000:
                    self.logger.warning("API tokenå³å°†ç”¨å°½ï¼Œåœæ­¢ç”Ÿæˆ")
                    break
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†æ ·æœ¬ {start_idx + i} æ—¶å‡ºé”™: {str(e)}")
                generated_titles.append("å¤„ç†å¼‚å¸¸")
                self.results['generation_metadata'].append({
                    'sample_idx': start_idx + i,
                    'error': str(e)
                })
        
        return generated_titles
    
    def generate_titles_batch(self, samples: List[Dict[str, Any]], 
                             batch_size: int = None) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆæ ‡é¢˜"""
        
        if batch_size is None:
            batch_size = self.config['data']['batch_size']
        
        generated_titles = []
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆæ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i+batch_size]
            
            try:
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}, æ ·æœ¬ {i} åˆ° {i+len(batch)}")
                
                # ç”Ÿæˆæ‰¹é‡æç¤ºè¯
                system_prompt, batch_prompt = self.prompt_generator.generate_batch_prompt(batch)
                
                # è°ƒç”¨LLMæ‰¹é‡ç”Ÿæˆ
                batch_titles = self.llm_client.generate_batch_titles(
                    batch,
                    system_prompt,
                    batch_prompt
                )
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                for j, title in enumerate(batch_titles):
                    if title:
                        generated_titles.append(title)
                    else:
                        generated_titles.append("æ‰¹é‡ç”Ÿæˆå¤±è´¥")
                    
                    self.results['generation_metadata'].append({
                        'sample_idx': i + j,
                        'batch_id': i//batch_size,
                        'title': title or "æ‰¹é‡ç”Ÿæˆå¤±è´¥"
                    })
                
                # æ£€æŸ¥APIä½¿ç”¨æƒ…å†µ
                usage_stats = self.llm_client.get_usage_stats()
                if usage_stats['remaining_tokens'] <= 2000:
                    self.logger.warning("API tokenå³å°†ç”¨å°½ï¼Œåœæ­¢ç”Ÿæˆ")
                    break
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1} æ—¶å‡ºé”™: {str(e)}")
                # ä¸ºè¯¥æ‰¹æ¬¡çš„æ‰€æœ‰æ ·æœ¬æ·»åŠ é”™è¯¯æ ‡è®°
                for j in range(len(batch)):
                    generated_titles.append("æ‰¹æ¬¡å¤„ç†å¼‚å¸¸")
                    self.results['generation_metadata'].append({
                        'sample_idx': i + j,
                        'error': str(e)
                    })
        
        return generated_titles
    
    def run_generation(self, mode: str = 'single', max_samples: int = None, 
                      force_reprocess: bool = False) -> bool:
        """è¿è¡Œæ ‡é¢˜ç”Ÿæˆæµç¨‹"""
        
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆ")
        self.logger.info("=" * 60)
        
        # 1. æ•°æ®å‡†å¤‡
        self.logger.info("æ­¥éª¤ 1: æ•°æ®å‡†å¤‡")
        if not self.load_or_process_data(force_reprocess):
            return False
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        samples = self.data_processor.test_samples
        if max_samples:
            samples = samples[:max_samples]
            self.logger.info(f"é™åˆ¶å¤„ç†æ ·æœ¬æ•°ä¸º: {max_samples}")
        
        # 2. æ ‡é¢˜ç”Ÿæˆ
        self.logger.info(f"æ­¥éª¤ 2: æ ‡é¢˜ç”Ÿæˆ (æ¨¡å¼: {mode})")
        
        if mode == 'single':
            generated_titles = self.generate_titles_single(samples)
        elif mode == 'batch':
            generated_titles = self.generate_titles_batch(samples)
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„ç”Ÿæˆæ¨¡å¼: {mode}")
            return False
        
        # 3. æ•´ç†ç»“æœ
        self.logger.info("æ­¥éª¤ 3: æ•´ç†ç»“æœ")
        self.results['generated_titles'] = generated_titles
        self.results['reference_titles'] = [s['reference_title'] for s in samples[:len(generated_titles)]]
        self.results['user_interests'] = [s['user_interests'] for s in samples[:len(generated_titles)]]
        self.results['news_categories'] = [s['category'] for s in samples[:len(generated_titles)]]
        # æ·»åŠ LLMè´¨é‡è¯„ä¼°æ‰€éœ€çš„æ•°æ®
        self.results['original_titles'] = [s['original_title'] for s in samples[:len(generated_titles)]]
        self.results['news_contents'] = [s['news_body'] for s in samples[:len(generated_titles)]]
        self.results['user_histories'] = [s['user_history'] for s in samples[:len(generated_titles)]]
        
        # 4. ä¿å­˜ç»“æœ
        self.save_results()
        
        # 5. æ‰“å°APIä½¿ç”¨ç»Ÿè®¡
        usage_stats = self.llm_client.get_usage_stats()
        self.logger.info("APIä½¿ç”¨ç»Ÿè®¡:")
        self.logger.info(f"  æ€»è¯·æ±‚æ•°: {usage_stats['total_requests']}")
        self.logger.info(f"  å¤±è´¥è¯·æ±‚æ•°: {usage_stats['failed_requests']}")
        self.logger.info(f"  æˆåŠŸç‡: {usage_stats['success_rate']:.2%}")
        self.logger.info(f"  æ€»tokenä½¿ç”¨: {usage_stats['total_tokens_used']}")
        self.logger.info(f"  å‰©ä½™token: {usage_stats['remaining_tokens']}")
        
        self.logger.info(f"æ ‡é¢˜ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(generated_titles)} ä¸ªæ ‡é¢˜")
        return True
    
    def save_results(self):
        """ä¿å­˜ç”Ÿæˆç»“æœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = os.path.join(
            self.config['paths']['generated_titles_dir'], 
            f'generation_results_{timestamp}.json'
        )
        
        try:
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ°: {results_path}")
            
            # ä¿å­˜ç®€åŒ–çš„æ ‡é¢˜å¯¹æ¯”æ–‡ä»¶
            comparison_path = os.path.join(
                self.config['paths']['generated_titles_dir'],
                f'title_comparison_{timestamp}.txt'
            )
            
            with open(comparison_path, 'w', encoding='utf-8') as f:
                f.write("ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆç»“æœå¯¹æ¯”\n")
                f.write("=" * 60 + "\n\n")
                
                for i, (gen, ref) in enumerate(zip(
                    self.results['generated_titles'], 
                    self.results['reference_titles']
                )):
                    f.write(f"æ ·æœ¬ {i+1}:\n")
                    f.write(f"ç”Ÿæˆæ ‡é¢˜: {gen}\n")
                    f.write(f"å‚è€ƒæ ‡é¢˜: {ref}\n")
                    f.write("-" * 40 + "\n")
            
            self.logger.info(f"æ ‡é¢˜å¯¹æ¯”æ–‡ä»¶å·²ä¿å­˜åˆ°: {comparison_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    def run_evaluation(self):
        """è¿è¡Œè¯„ä¼°æµç¨‹"""
        
        if not self.results['generated_titles']:
            self.logger.error("æ²¡æœ‰ç”Ÿæˆç»“æœå¯ä¾›è¯„ä¼°")
            return False
        
        self.logger.info("å¼€å§‹è¯„ä¼°ç”Ÿæˆç»“æœ...")
        
        # æ‰§è¡Œç»¼åˆè¯„ä¼°
        evaluation_results = self.evaluator.comprehensive_evaluation(self.results)
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(
            self.config['paths']['evaluation_results_dir'],
            f'evaluation_report_{timestamp}.txt'
        )
        
        report = self.evaluator.generate_evaluation_report(evaluation_results, report_path)
        print("\n" + report)
        
        # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
        detailed_results_path = os.path.join(
            self.config['paths']['evaluation_results_dir'],
            f'detailed_evaluation_{timestamp}.json'
        )
        self.evaluator.save_detailed_results(detailed_results_path)
        
        # ç”Ÿæˆè¯„ä¼°å›¾è¡¨
        try:
            chart_path = os.path.join(
                self.config['paths']['evaluation_results_dir'],
                f'evaluation_chart_{timestamp}.png'
            )
            self.evaluator.create_comparison_chart(save_path=chart_path)
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆè¯„ä¼°å›¾è¡¨å¤±è´¥: {e}")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description='åŸºäºæç¤ºè¯å·¥ç¨‹çš„ä¸ªæ€§åŒ–æ–°é—»æ ‡é¢˜ç”Ÿæˆ')
    parser.add_argument('--mode', choices=['single', 'batch'], default='single',
                       help='ç”Ÿæˆæ¨¡å¼: single(é€ä¸ªç”Ÿæˆ) æˆ– batch(æ‰¹é‡ç”Ÿæˆ)')
    parser.add_argument('--max-samples', type=int, default=50,
                       help='æœ€å¤§å¤„ç†æ ·æœ¬æ•°é‡')
    parser.add_argument('--force-reprocess', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°å¤„ç†æ•°æ®')
    parser.add_argument('--skip-generation', action='store_true',
                       help='è·³è¿‡ç”Ÿæˆï¼Œä»…è¿è¡Œè¯„ä¼°')
    parser.add_argument('--skip-evaluation', action='store_true',
                       help='è·³è¿‡è¯„ä¼°ï¼Œä»…è¿è¡Œç”Ÿæˆ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = PersonalizedTitleGenerator()
    
    success = True
    
    # è¿è¡Œç”Ÿæˆæµç¨‹
    if not args.skip_generation:
        success = generator.run_generation(
            mode=args.mode,
            max_samples=args.max_samples,
            force_reprocess=args.force_reprocess
        )
    
    # è¿è¡Œè¯„ä¼°æµç¨‹
    if success and not args.skip_evaluation:
        generator.run_evaluation()
    
    if success:
        print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“ æŸ¥çœ‹ç»“æœæ–‡ä»¶å¤¹: {DATA_PATHS['output_dir']}")
    else:
        print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")

if __name__ == "__main__":
    main() 