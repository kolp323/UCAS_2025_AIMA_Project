import os
import torch
from datasets import load_dataset
from transformers import (
    BartForConditionalGeneration,
    BartTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)

# === é…ç½®å‚æ•° ===
MODEL_NAME = "facebook/bart-base"
CSV_PATH = "../data/PENS/train.tsv"  # æ”¹æˆä½ çš„è·¯å¾„
TEXT_COLUMN = "content"
TITLE_COLUMN = "title"

# === 1. åŠ è½½æ•°æ®é›† ===
print("ğŸ”¹ Loading dataset...")
dataset = load_dataset("csv", data_files=CSV_PATH)["train"]
# # æ‰“å°åˆ—åç”¨äºè°ƒè¯•
# print("Columns in dataset:", dataset.column_names)
# Columns in dataset: ['UserID\tClicknewsID\tdwelltime\texposure_time\tpos\tneg\tstart\tend\tdwelltime_pos']

# # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬æŸ¥çœ‹å®é™…å†…å®¹
# print("First example:", dataset[0])

# === 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ===
print("ğŸ”¹ Loading tokenizer and model...")
tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)
model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

# === 3. é¢„å¤„ç†å‡½æ•° ===
def preprocess(example):
    model_inputs = tokenizer(
        example[TEXT_COLUMN],
        max_length=512,
        padding="max_length",
        truncation=True
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example[TITLE_COLUMN],
            max_length=64,
            padding="max_length",
            truncation=True
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# === 4. Tokenize Dataset ===
print("ğŸ”¹ Tokenizing dataset...")
tokenized_dataset = dataset.map(preprocess, batched=True)

# === 5. è®­ç»ƒé…ç½® ===
training_args = TrainingArguments(
    output_dir="./bart_pens_output",
    evaluation_strategy="no",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_total_limit=2,
    fp16=torch.cuda.is_available(),
    logging_steps=20,
    save_strategy="epoch",
    learning_rate=5e-5,
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# === 6. Trainer å¾®è°ƒ ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

print("ğŸš€ Starting training...")
trainer.train()

# === 7. ä¿å­˜æ¨¡å‹ ===
print("ğŸ’¾ Saving model...")
model.save_pretrained("./bart_pens_model")
tokenizer.save_pretrained("./bart_pens_model")

# === 8. æ¨ç†æµ‹è¯•å‡½æ•° ===
def generate_title(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(model.device)
    summary_ids = model.generate(inputs["input_ids"], max_length=64)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# ç¤ºä¾‹
test_text = "5æœˆ27æ—¥ï¼ŒåŒ—äº¬å‘ç”Ÿå¤§è§„æ¨¡äº¤é€šæ‹¥å µï¼Œæ•°ç™¾è¾†è½¦æ’é˜Ÿè¶…è¿‡ä¸¤ä¸ªå°æ—¶ã€‚"
print("ğŸ“ Example:")
print("Input:", test_text)
print("Generated Title:", generate_title(test_text))
